# LLMParser: An Exploratory Study on Using Large Language Models for Log Parsing

## Abstract
System logs have been extensively utilized as valuable data for ana- lyzing and diagnosing system failures. The fundamental task in log processing is log parsing, which involves converting unstructured logs into structured form. The existing approaches can be catego- rized into two groups. The first group comprises non-deep learning approaches that cluster or partition logs based on statistical fea- tures, while they overlook the crucial identification of constants and variables. This oversight can result in inaccurate variable identifica- tion, leading to subpar performance in downstream tasks. The other approach involves deep-learning-based approaches, which leverage neural networks’ capabilities to effectively identify variables and constants, leading to high accuracy. However, these approaches often suffer from limited scalability. When dealing with logs outside the training data, their parsing performance tends to be sub-optimal. Meanwhile their efficiency can also be compromised. In this paper, we present a novel log parsing approach named Hooglle, designed to address these challenges. Leveraging the vast knowledge within a large language model, Hooglle extracts tem- plates, enabling high-precision log parsing and achieving remark- able scalability. To tackle the inefficiencies arising from the large language model, we integrate existing templates into a character- level prefix tree called CI-tree, significantly improving parsing ef- ficiency. We extensively evaluate Hooglle on diverse real-world datasets and demonstrate its impressive performance on 16 publicly labeled benchmark datasets.

## Structure
We present LLMparser repository structure below.

```
.
├── LLMs
│   ├── chatglm.sh
│   ├── flan-t5-base.sh
│   ├── flan-t5-small.sh
│   └── llama.sh
├── README.md
├── chatglm
│   ├── eval.py
│   ├── modeling_chatglm.py
│   ├── run_copy.sh
│   └── train.py
├── data_sampling.py
├── docs
├── env_init.sh
├── evaluate
│   └── evaluator.py
├── fine_tuned_model
├── flan-t5
│   └── train.py
├── llama
│   ├── 1.py
│   ├── Dockerfile
│   ├── cross_eval.py
│   ├── docker-compose.yml
│   ├── eval.py
│   ├── export.sh
│   ├── export_hf_checkpoint.py
│   ├── finetune.py
│   ├── pyproject.toml
│   ├── run.sh
│   └── utils
│       ├── README.md
│       ├── __init__.py
│       ├── __pycache__
│       │   ├── __init__.cpython-39.pyc
│       │   └── prompter.cpython-39.pyc
│       ├── callbacks.py
│       └── prompter.py
├── logs
│   └── ...
├── output
└── requirements.txt
```


## Environment 
### Requirement
```shell
sh env_init.sh
```

### Large Language Models

To download the Large Language Models:
```shell
cd LLMs
sh flan-t5-base.sh
```


## Data sampling

Sample 50 logs from Mac dataset
```shell
python train.py --project "Mac" \
                --shot 50
```


## Fine-tune and Inference

Flan-T5-base or Flan-T5-small (fine-tuned with 50 shot)
```shell
cd flan-t5
python train.py --model "flan-t5-base"\
                --num_epochs 30 \
                --learning_rate 5e-4 \
                --train_percentage "cross" \
                --validation "validation" \
                --systems "Mac,Android,Thunderbird,HealthApp,OpenStack,OpenSSH,Proxifier,HPC,Zookeeper,Hadoop,Linux,HDFS,BGL,Windows,Apache,Spark"
```

LLaMA (fine-tuned with 50 shot)
```shell
cd llama
sh run.sh 0.025
```

ChatGLM (fine-tuned with 50 shot)
```shell
cd chatglm
sh run.sh 0.025
```

## Evaluation

Evaluate LLM parsing result on certain training dataset size (Flan-T5-base result on 50 shots)
```shell
cd evaluate
python evaluator.py --model "flan-t5-base" \
    --train_percentage "0.025" \
    --systems "Mac,Android,Thunderbird,HealthApp,OpenStack,OpenSSH,Proxifier,HPC,Zookeeper,Hadoop,Linux,HDFS,BGL,Windows,Apache,Spark" 
```

## Evaluation Results
### RQ1: What is the accuracy of LLM?
<p align="center"><img src="docs/tab2.png" width="800"></p>

### RQ2: How does the accuracy of log parsing vary under different shot sizes?
<p align="center"><img src="docs/tab3.png" width="800"></p>
<p align="center"><img src="docs/tab6.png" width="500"></p>

### RQ3: How is the generalizability of LLMParsers on unseen log templates?
<p align="center"><img src="docs/tab4.png" width="800"></p>

### RQ4: Can pre-trained LLMParsers help improve parsing accuracy?
<p align="center"><img src="docs/tab5.png" width="800"></p>
